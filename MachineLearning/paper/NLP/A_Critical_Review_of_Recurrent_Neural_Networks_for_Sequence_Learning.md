## ref

* [1506.00019](https://arxiv.org/pdf/1506.00019)  原文

* [Understanding LSTM Networks -- colah's blog](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)  一篇很好的blog

* [BPTT算法详解：深入探究循环神经网络（RNN）中的梯度计算【原理理解】-CSDN博客](https://blog.csdn.net/qq_22841387/article/details/139283146)

* [TBPTT算法——Truncated Backpropagation Through Time-CSDN博客](https://blog.csdn.net/Answer3664/article/details/101059171)

## Introduction

这边介绍了RNN相比传统网络的一个优势：

* 传统网络理论上要求每个训练集和测试机的数据点都是独立的，因为每处理一个数据点后网络的状态都会丢失（这里意思应该是说传统NN没有记忆组件）

* RNN则有能力在序列的每步中传递部分信息

引入RNN时作者提了三个关键问题并一一解答

* 为什么模型需要显式地对序列进行建模
  
  传统模型没有对序列建模，但一般会通过滑动窗口等引入上下文的概念。但滑动窗口的一个致命缺点就是无法处理长上下文

* 为什么不用马尔可夫模型
  
  因为随着上下文窗口的增加，马尔可夫模型的转移矩阵会指数级爆炸，特别在底数很大的情况下爆炸情况更严重
  
  此外RNN的隐含层表达能力很强，即使假设隐含层中每个神经元都只有1和0两个值，其也可以表达2^N种状态；而一般神经元状态都是由实数表示的，可以表示的状态非常多

* RNN的表达能力会过强么
  
  这边提到了一个有限长度RNN可以模拟一个通用图灵机，但这里可能导致RNN训练过程难以收敛。但一个确定架构的RNN可以采用常见的梯度下降来进行训练，并且通过一些常用方法，如权重衰减、dropout、限制自由度等方式防止过拟合

## Background

### Sequences

RNN的输入输出都是序列，可以用 $(x^{(1)}, x^{(2)},...,x^{(T)})$ 表示输入，$(y^{(1)}, y^{(2)},...,y^{(T)})$ 表述输出

这里每个序列数据的间隔可以是以时间为间隔的，也可以是其他。比如一句话的每个单词也可以作为序列的每个元素输入

### Neural networks

基本是神经网络结构的基础知识

### Feedforward networks and backpropagation

反向传播的基本知识

### Recurrent neural networks

![](pic/1_1.png)

循环神经网络输出的公式为

$$
h^{(t)} = \sigma (W^{hx} x^{(t)} + W^{hh} h^{(t-1)} + b_h)
\\
\hat y^{(t)} = softmax(W^{yh}h^{(t)} + b_y)
$$

其中$W^{hx}$是输入和隐含层的权重转换矩阵，$W^{hh}$是隐含层内部相邻时间步长间的权重转换矩阵，bh和by是偏移

下图是一个简单的循环神经网络

![](pic/1_2.png)

若按时间步长展开，可以视为下图

![](pic/1_3.png)

展开后可以视为一个深度神经网络，每个时间步长对应一个隐含层，且每个时间步长间的权重共享。因此该网络可以使用反向传播训练，这种算法被称为backpropagation through time（BPTT）

#### Early recurrent network designs

最早的循环神经网络是Hopfield（1982）定义的，用于模式识别。该网络定义了一系列节点，并在节点间有一些权重值，链接函数则是简单的零阈值。使用时将模式编码为节点中的值，网络根据其更新规则执行几轮，最后输出另一个模式。该网络适用于从损坏的模式恢复原有模式，是玻尔兹曼机和自动编码机的前身

另一个早期的循环神经网络由Jordan（1986）引入，是一个包含一个隐含层，且扩展了一些特殊单元的前馈神经网络。输出节点会反馈到特殊单元，这些特殊单元的值则会在下一个时间步中反馈到隐含层，因此特殊单元可以记录上一个时间步的动作。此外，特殊单元也有一条反馈函数连接到节点本身上，因此也使得该网络可以跨多个时间步长传递信息。Sutskever（2014）也使用了一个类似直接由输出节点接入反馈的方法设计模型，该模型用于自然语言翻译，当生成一段文本时，每个时间步生成的单词都会在下一个时间步作为输入反馈到网络中

![](pic/1_4.png)

Elman（1990）使用了比Jordan简单的网络结构：与隐含层中每个单元关联的是一个上下文单元，每个上下文单元j'将对应的隐含层单元j上一个时间步的状态作为输入，并且采用一个固定权重$w_{j'j} = 1$；上下文单元的输出之后又通过一条标准边反馈到对应的隐含层单元j'

这种结构已经等价于简单的RNN，因为每个隐含层单元都有一条自连接的循环边。通过一条固定权重的循环边使得隐含层节点自连接的设计是LSTM的基础。

这篇论文做了一个实验：输入一个二进制序列，其中前两个数字是随机的，最后一个数字是前两个数字的xor值。随机猜测的正确率为50%，该网络正确率能达到66.7%，在当时算表现最好的网络了

#### 训练循环神经网络

训练循环神经网络在很长一段时间都被认为是困难问题，因为需要解决如何跨时间步反向传播目标函数偏差并且进行梯度下降的问题

这里考虑一个最简单的循环神经网络：

![](pic/1_5.png)

现在考虑输入在τ时间传给网络，而偏差在t时间计算出来，并假设中间的时间步输入为0。循环神经网络的梯度存在一个重要的问题，即梯度爆炸或梯度消失。考虑将上述循环神经网络按时间步展开

![](pic/1_6.png)

由于循环神经网络有一条自连接边，因此上述展开中每个时间步间的边权重都是一致的，因此输入值传播到输出中间会经过n次同样的边，因此梯度要么指数上升（$|w_{j'j}|>1$），要么指数下降（$|w_{j'j}|<1$）。这个问题会随着时间步的增加而越发严重

论文第17页讲述了一系列对RNN训练方法的研究

循环神经网络常用训练方法BPTT：[BPTT算法详解：深入探究循环神经网络（RNN）中的梯度计算【原理理解】-CSDN博客](https://blog.csdn.net/qq_22841387/article/details/139283146)

TBPTT（Truncated backpropagation through time）用于解决BPTT对于长时间步开销过大的问题：[TBPTT算法——Truncated Backpropagation Through Time-CSDN博客](https://blog.csdn.net/Answer3664/article/details/101059171)

### Modern RNN Architectures

现代RNN网络架构主要由两篇文章引入，一个是1997年的LSTM（Long Short-Term Memory），引入了存储单元，一种用于替代RNN隐含层中传统单元的计算单元，使用这种单元可以克服早期RNN训练上面临的一系列困难

第二篇是1997年的BRNN（Bidirectional Recurrent Neural Networks），该文章介绍了一种架构的RNN，它的输出不仅依赖于过去的序列，还依赖于未来的序列（与传统结构只依赖过去数据不同），该架构成功运用于自然语言处理

#### LSTM

![](pic/1_7.png)

如图是一个LSTM单元，自连接的节点代表了内部状态s，这是一个线性单元。蓝色虚线是循环边，该边有固定的权重。LSTM具体由下列部件组成

* 输入节点：图中的gc，该节点以当前输入层的值x(t)和上一个隐含层的值h(t-1)作为输入。激活函数一般是tanh，但在提出LSTM的论文中是sigmoid
  
  $$
  \boldsymbol g^{(t)} = \phi( W^{gx} \boldsymbol x^{(t)} + W^{gh} \boldsymbol h^{(t-1)} + \boldsymbol b_g )
  $$

* 输入门：输入门ic是LSTM中比较特殊的节点，也以当前输入和上一个时间步的隐含层作为输入。它称为门是因为它会与gc相乘，因此若ic为0，则等同于阻断输入，为1则等同于接收所有输入
  
  $$
  i^{(t)} = \sigma(W^{ix} \boldsymbol x^{(t)} + W^{ih} \boldsymbol h^{(t-1)} + \boldsymbol b_i)
  $$

* 内部状态：内存单元的核心sc是一个线性节点，输入为g和i的点积。其有一条固定权重的自连接边。这条边虽然跨越了多个时间步但是权重为常数，因此不会导致梯度爆炸或梯度消失
  
  $$
  \boldsymbol s^{(t)} = \boldsymbol g^{(t)} \odot \boldsymbol i^{(t)} + \boldsymbol s^{(t-1)}
  $$

* 输出门：与输入门有类似的作用，其输入也与输入门一致
  
  $$
  \boldsymbol o^{(t)} = \sigma (W^{ox} \boldsymbol x^{(t)} + W^{oh} \boldsymbol h^{(t-1)} + \boldsymbol b_o)
  $$

* 最终输出：
  
  $$
  \boldsymbol h^{(t)} = \phi (\boldsymbol s^{(t)}) \odot \boldsymbol o^{(t)}
  $$

直观上考虑LSTM的各个单元：LSTM可以学习何时将一些动作放入内部状态，何时将其放出。当输入门为0时，没有动作可以进入，当输出门为0时没有动作可以输出。当两个门同时关闭时状态会在内部流转。而内部状态自连接边权重为常数的设计使得不会出现梯度爆炸或梯度消失的问题

LSTM出现后也诞生了一些变体，如遗忘门的设计，该设计已经基本成为现在LSTM的标准

![](pic/1_8.png)

* 遗忘门：该单元的引入使得网络可以学习如何更新内部状态，这对于一个连续运行的网络尤其有用
  
  $$
  \boldsymbol f^{(t)} = \sigma (W^{fx} \boldsymbol x^{(t)} + W^{fh} \boldsymbol h^{(t-1)} + \boldsymbol b_f)
\\
\boldsymbol s^{(t)} = \boldsymbol g^{(t)} \odot \boldsymbol i^{(t)} + \boldsymbol f^{(t)} \odot \boldsymbol s^{(t-1)}
  $$

* 还有一种变体是将一些内部状态直接"peephole connect"到输入和输出门（这些连接都是在同一个LSTM单元中）。对于这种设计有一个直观的理解：考虑网络针对的任务是计数，并且当计数了n个对象时会产生对应输出。网络可能会学习在看到每个对象时让固定数量的动作进入内部状态sc，并且每次看到一个对象sc都会迭代递增；而当看到第n个对象时，网络需要让一些内容从内部状态输出，从而来影响网络输出的内容，为了实现这一点输出门oc必须知道内部状态sc的内容

下面展示使用LSTM单元替代RNN中隐含层的普通单元的例子

![](pic/1_3.png)

![](pic/1_9.png)

实际上就是把原来隐含层节点的输出接到对应LSTM单元的各个需要上一个时间节点隐含层作为输入的地方。这些结构的共同点就是每一层都以上一层当前输入值和当前层的上一个时间步值作为输入

#### BRNN

除LSTM外另一种最常用的RNN架构是BRNN，该架构有两个隐含层，每个隐含层都会跟输入和输出连接，第一个隐含层与普通RNN相同，节点与节点间按时间正向连接，而第二个隐含层则相反，如图

![](pic/1_10.png)

给定输入序列和输出序列，BRNN可以在时间步展开后运行反向传播算法进行训练。BRNN的正向传播算法如下：

$$
\boldsymbol h^{(t)} = \sigma(W^{hx} \boldsymbol x^{(t)} + W^{hh} \boldsymbol h^{(t-1)} + \boldsymbol b_h)
\\
\boldsymbol z^{(t)} = \sigma(W^{zx} \boldsymbol x^{(t)} + W^{zz} \boldsymbol h^{(t+1)} + \boldsymbol b_z)
\\
\hat{\boldsymbol y}^{(t)} = softmax(W^{yh} \boldsymbol h^{(t)} + W^{yz} \boldsymbol z^{(t)} + \boldsymbol b_y)
$$

其中h和z是隐含层中正向连接和反向连接的传播值

BRNN有一个限制，就是它的输入必须有一个固定的点，换句话说它必须获取一定的过去和未来的输入才能进行输出（我觉得可以理解为输出有一定的延迟）。并且对于一些只能以过去的数据作为输入的任务不适用

LSTM和BRNN实际上思路类似：通过替换RNN隐含层中的基础单元来实现优化。实际上一种称为BLSTM的网络也在手写识别上达到SOTA（当时的数据）

#### Neural Turing Machines

有点类似图灵机，结构是一个RNN扩展了可寻址的内存。

神经网络图灵机的两个主要组件：

* 控制器  循环前馈神经网络，其获取输入并产生输出，且负责将指令从内存读取或写入

* 内存矩阵  N*M的内存单元，存在一定数量的读写头供控制器进行交互

NTM仍然具有端到端的可分性，并可以由BPTT进行训练

NTM的实验是模拟一些基础的图灵机算法，如复制、排序等，作者使用了几种网络进行训练：LSTM RNN、使用前馈控制器的NTM、使用LSTM控制器的NTM。结果表明两种NTM结构都显著优于LSTM RNN

#### Applications of LSTMs and BRNNs
